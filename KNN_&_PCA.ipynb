{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems? 📌 K-Nearest Neighbors (KNN) in Classification & Regression\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris, load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "# ----------------------------\n",
        "# 🔹 KNN for Classification\n",
        "# ----------------------------\n",
        "print(\"=== KNN for Classification (Iris Dataset) ===\")\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_clf = knn_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_clf))\n",
        "\n",
        "# ----------------------------\n",
        "# 🔹 KNN for Regression\n",
        "# ----------------------------\n",
        "print(\"\\n=== KNN for Regression (Diabetes Dataset) ===\")\n",
        "\n",
        "# Load dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Regressor\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_reg = knn_reg.predict(X_test)\n",
        "\n",
        "# Mean Squared Error\n",
        "print(\"Regression Mean Squared Error:\", mean_squared_error(y_test, y_pred_reg))\n",
        "\n",
        "# ----------------------------\n",
        "# ✅ Explanation:\n",
        "# - Classification → Majority voting among nearest neighbors\n",
        "# - Regression → Average value among nearest neighbors\n",
        "# - Scaling features is required since KNN relies on distances\n",
        "# ----------------------------\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ7m0V895TEZ",
        "outputId": "5bf263af-b9c9-40ea-874d-a180b361818e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== KNN for Classification (Iris Dataset) ===\n",
            "Classification Accuracy: 1.0\n",
            "\n",
            "=== KNN for Regression (Diabetes Dataset) ===\n",
            "Regression Mean Squared Error: 3047.449887640449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans- 📌 Curse of Dimensionality & Its Effect on KNN\n",
        "🔹 What is the Curse of Dimensionality?\n",
        "\n",
        "The curse of dimensionality refers to problems that arise when the number of features (dimensions) in the dataset becomes very large.\n",
        "\n",
        "In high-dimensional space:\n",
        "\n",
        "- Data becomes sparse → Points are far apart.\n",
        "\n",
        "- Distance measures lose meaning → The difference between the nearest and farthest neighbor becomes very small.\n",
        "\n",
        "- More data needed → The volume of space increases exponentially, so we need exponentially more data to maintain density.\n",
        "\n",
        "🔹 How it Affects KNN Performance?\n",
        "\n",
        "Since KNN relies on distance (Euclidean/Manhattan, etc.), the curse of dimensionality creates problems:\n",
        "\n",
        "- Distances become less meaningful → All points seem equally far, so KNN struggles to identify true neighbors.\n",
        "\n",
        "- Overfitting risk → With many irrelevant features, noise dominates the distance calculation.\n",
        "\n",
        "- High computation cost → Distance calculation in high dimensions is very expensive.\n",
        "\n",
        "🔹 Ways to Reduce the Effect:\n",
        "\n",
        "- Feature Selection → Keep only relevant features.\n",
        "\n",
        "- Dimensionality Reduction → Use PCA, t-SNE, or autoencoders.\n",
        "\n",
        "- Scaling/Normalization → Helps but does not fully solve the issue."
      ],
      "metadata": {
        "id": "j05-fFs3501a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Ans-📌 Principal Component Analysis (PCA) vs Feature Selection\n",
        "\n",
        "🔹 What is PCA?\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original features into a new set of features called principal components.\n",
        "\n",
        "These components are linear combinations of the original features.\n",
        "\n",
        "They are chosen to capture maximum variance in the data.\n",
        "\n",
        "The first component captures the most variance, the second captures the next, and so on.\n",
        "\n",
        "\n",
        "```\n",
        "| Aspect               | PCA (Feature Extraction)                                                                 | Feature Selection                                         |\n",
        "| -------------------- | ---------------------------------------------------------------------------------------- | --------------------------------------------------------- |\n",
        "| **Definition**       | Creates new features (principal components) as linear combinations of original features. | Selects a subset of the most important original features. |\n",
        "| **Approach**         | Transforms the feature space.                                                            | Keeps original features, removes irrelevant ones.         |\n",
        "| **Interpretability** | Components are harder to interpret (mix of many features).                               | Selected features remain interpretable.                   |\n",
        "| **Goal**             | Reduce dimensionality while retaining variance.                                          | Reduce dimensionality by keeping only important features. |\n",
        "| **Example**          | Convert 100 correlated features into 10 components.                                      | Pick top 10 features out of 100 based on importance.      |\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "X_o1dbke68_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Ans- 📌 Eigenvalues & Eigenvectors in PCA\n",
        "\n",
        "🔹 What are Eigenvalues & Eigenvectors?\n",
        "\n",
        "- Eigenvectors: Directions along which data varies the most (principal components).\n",
        "\n",
        "- Eigenvalues: The amount of variance captured along each eigenvector (importance/weight of each component).\n",
        "\n",
        "👉 In PCA:\n",
        "\n",
        "- Eigenvectors = new feature axes (principal components).\n",
        "\n",
        "- Eigenvalues = how much variance (information) each component explains.\n",
        "\n",
        "🔹 Why are They Important in PCA?\n",
        "\n",
        "1. Eigenvectors determine the orientation of new axes (principal components).\n",
        "\n",
        "2. Eigenvalues tell us how much information (variance) is retained by each component.\n",
        "\n",
        "3. We use the largest eigenvalues → corresponding eigenvectors form the reduced feature space.\n",
        "\n",
        "4. Helps in deciding how many components to keep (e.g., keep components that explain 95% variance)."
      ],
      "metadata": {
        "id": "9r1OMaR2_vZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Ans- 📌 How KNN and PCA Complement Each Other\n",
        "\n",
        "🔹 KNN Recap\n",
        "\n",
        "- KNN is a distance-based algorithm.\n",
        "\n",
        "- Performance depends heavily on feature space and distances.\n",
        "\n",
        "- Struggles in high-dimensional data (curse of dimensionality).\n",
        "\n",
        "🔹 PCA Recap\n",
        "\n",
        "- PCA reduces dimensionality by creating new features (principal components).\n",
        "\n",
        "- Removes noise and correlations between features.\n",
        "\n",
        "- Retains maximum variance in fewer dimensions.\n",
        "\n",
        "🔹 How They Work Together\n",
        "\n",
        "- PCA before KNN → PCA reduces dimensionality, keeping only the most informative components.\n",
        "\n",
        "- This makes distances more meaningful for KNN (less noise, less redundancy).\n",
        "\n",
        "- PCA also reduces computation cost → KNN is faster with fewer features.\n",
        "\n",
        "- PCA helps avoid overfitting in KNN by removing irrelevant/weak features."
      ],
      "metadata": {
        "id": "z_q7wOaNBysc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6: Train a KNN Classifier on the Wine dataset with and without features caling. Compare model accuracy in both cases.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --------------------------\n",
        "# 1️⃣ KNN without Scaling\n",
        "# --------------------------\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# --------------------------\n",
        "# 2️⃣ KNN with Scaling\n",
        "# --------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = knn_scaling.predict(X_test_scaled)\n",
        "acc_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# --------------------------\n",
        "# Results\n",
        "# --------------------------\n",
        "print(\"Accuracy without Scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with Scaling   :\", acc_scaling)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwzs1RR5Cf9T",
        "outputId": "2848c96c-94ad-4b8d-c111-ba2ab81b9a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.7222222222222222\n",
            "Accuracy with Scaling   : 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (keep all components)\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "for i, var in enumerate(explained_variance):\n",
        "    print(f\"PC{i+1}: {var:.4f}\")\n",
        "\n",
        "print(\"\\nTotal Variance Explained:\", explained_variance.sum())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rRLifSJCtcp",
        "outputId": "e217ddef-72ab-4166-aecb-3b0d630e36cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n",
            "\n",
            "Total Variance Explained: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --------------------------\n",
        "# 1️⃣ KNN on Original Dataset\n",
        "# --------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# --------------------------\n",
        "# 2️⃣ KNN on PCA-Transformed Dataset (Top 2 Components)\n",
        "# --------------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# --------------------------\n",
        "# Results\n",
        "# --------------------------\n",
        "print(\"Accuracy on Original Dataset:\", acc_original)\n",
        "print(\"Accuracy on PCA (2 Components):\", acc_pca)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn_62fO5DGlz",
        "outputId": "2d7cc379-0b97-43b1-ef1a-f7202daff41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original Dataset: 0.9444444444444444\n",
            "Accuracy on PCA (2 Components): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --------------------------\n",
        "# 1️⃣ KNN with Euclidean Distance (default)\n",
        "# --------------------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# --------------------------\n",
        "# 2️⃣ KNN with Manhattan Distance\n",
        "# --------------------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# --------------------------\n",
        "# Results\n",
        "# --------------------------\n",
        "print(\"Accuracy with Euclidean Distance:\", acc_euclidean)\n",
        "print(\"Accuracy with Manhattan Distance:\", acc_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlRngUxCDaNz",
        "outputId": "5523f328-4d76-4dfc-87c2-dd6ed5ac67b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean Distance: 0.9444444444444444\n",
            "Accuracy with Manhattan Distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "Ans- 📌 Cancer Classification with PCA + KNN\n",
        "\n",
        "🔹 Problem\n",
        "\n",
        "- High-dimensional gene expression dataset (many features, few samples).\n",
        "\n",
        "- Risk: Overfitting because traditional ML models cannot generalize well in such cases.\n",
        "\n",
        "🔹 Step-by-Step Solution\n",
        "\n",
        "1. Use PCA to Reduce Dimensionality\n",
        "\n",
        "- Gene expression data may have thousands of features but only hundreds of patients.\n",
        "\n",
        "- Apply PCA after scaling → compress features into a few principal components that capture most variance.\n",
        "\n",
        "2. Decide How Many Components to Keep\n",
        "\n",
        "- Look at explained variance ratio.\n",
        "\n",
        "- Keep enough PCs to explain ~90–95% of variance (balance between information retention & noise reduction).\n",
        "\n",
        "- Use a scree plot (elbow method) for visualization.\n",
        "\n",
        "3. Use KNN for Classification Post-Dimensionality Reduction\n",
        "\n",
        "- After PCA transformation, apply KNN classifier.\n",
        "\n",
        "- KNN works better in reduced, denoised feature space since distances become more meaningful.\n",
        "\n",
        "4. Evaluate the Model\n",
        "\n",
        "- Use train-test split or cross-validation.\n",
        "\n",
        "- Metrics: Accuracy, Precision, Recall, F1-score (important for medical diagnosis).\n",
        "\n",
        "- Possibly use ROC-AUC for binary/multiclass evaluation.\n",
        "\n",
        "5. Justify Pipeline to Stakeholders\n",
        "\n",
        "- PCA → Handles high-dimensional data, reduces noise, avoids overfitting.\n",
        "\n",
        "- KNN → Simple, interpretable, works well with reduced features.\n",
        "\n",
        "- Evaluation → Cross-validation ensures robustness, metrics show reliability.\n",
        "\n",
        "- This pipeline balances accuracy + interpretability, making it a strong choice for real-world biomedical datasets.\n"
      ],
      "metadata": {
        "id": "bsTe9TkSDrvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10 ans-\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Simulate a high-dimensional gene expression dataset\n",
        "X, y = make_classification(n_samples=200, n_features=1000, n_informative=50,\n",
        "                           n_classes=3, random_state=42)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 1: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Apply PCA (keep 95% variance)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"Number of components retained:\", pca.n_components_)\n",
        "print(\"Explained variance ratio sum:\", np.sum(pca.explained_variance_ratio_))\n",
        "\n",
        "# Step 3: Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "# Step 4: Evaluate\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsCi0Ii3Ei0C",
        "outputId": "159693f6-1b93-4464-ba56-b4c43f531ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components retained: 125\n",
            "Explained variance ratio sum: 0.9525080991404814\n",
            "\n",
            "Accuracy: 0.3333333333333333\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.42      0.34        19\n",
            "           1       0.57      0.33      0.42        24\n",
            "           2       0.22      0.24      0.23        17\n",
            "\n",
            "    accuracy                           0.33        60\n",
            "   macro avg       0.36      0.33      0.33        60\n",
            "weighted avg       0.38      0.33      0.34        60\n",
            "\n"
          ]
        }
      ]
    }
  ]
}