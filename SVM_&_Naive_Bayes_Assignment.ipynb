{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9b6b0f-4a7d-46f5-b76a-f9acd1088235",
      "metadata": {
        "id": "6f9b6b0f-4a7d-46f5-b76a-f9acd1088235"
      },
      "outputs": [],
      "source": [
        "#what is a  sector support machine(svm),and how does it work?\n",
        "\n",
        "'''\n",
        "->A Support Vector Machine (SVM) is a supervised learning algorithm typically used for classification (and sometimes for regression) that aims to find the best possible decision boundary—a hyperplane—between data points of different classes by maximizing the margin between them\n",
        "\n",
        "How SVM Works\n",
        "1. Linear SVM & Maximum Margin\n",
        "SVM finds the hyperplane that maximizes the margin—the distance between the boundary and the closest training points from each class. These closest points are called support vectors\n",
        "\n",
        "2. Soft Margin & Hinge Loss\n",
        "Real-world data often overlaps. Soft‑margin SVM allows some misclassification or margin violations to improve generalization.\n",
        "\n",
        "3. Kernel Trick for Non‑Linear Data\n",
        "When data isn’t linearly separable in the original feature space, SVM uses a kernel function to implicitly map it into a higher-dimensional space where separation is easier.\n",
        "\n",
        "4. Training via Optimization\n",
        "The SVM solves a convex quadratic optimization problem (often via dual formulation using Lagrange multipliers) to determine which data points become support vectors and define the hyperplane\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d766be7-649f-4a28-a971-7bd40cc1ab2c",
      "metadata": {
        "id": "4d766be7-649f-4a28-a971-7bd40cc1ab2c"
      },
      "outputs": [],
      "source": [
        "#Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "'''\n",
        "->Hard Margin SVM\n",
        "Definition: Used when data is perfectly linearly separable—every point can lie outside the margin with no misclassification allowed.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Maximizes margin for perfect separation\n",
        "Unique, well-defined solution\n",
        "\n",
        "Cons:\n",
        "\n",
        "Fails when data is not linearly separable\n",
        "Highly sensitive to outliers or noise—just one bad point destroys feasibility\n",
        "\n",
        "Soft-Margin SVM\n",
        "Designed for real-world datasets that may be noisy or not perfectly separable.\n",
        "Trade-off parameter (C):\n",
        "\n",
        "Large C → penalizes misclassification heavily → nearby to hard margin\n",
        "\n",
        "Small C → allows more violations → wider margin and better generalization\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e991b082-0874-4c06-a2ef-26620c22bb14",
      "metadata": {
        "id": "e991b082-0874-4c06-a2ef-26620c22bb14"
      },
      "outputs": [],
      "source": [
        "#What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "'''\n",
        "->The Kernel Trick is a powerful technique used in Support Vector Machines (SVMs) to enable non-linear classification without explicitly mapping data to high-dimensional feature spaces.\n",
        "\n",
        "Example Kernel: Radial Basis Function (RBF)\n",
        "When to Use It\n",
        "Ideal for highly non-linear and complex datasets with unknown structure.\n",
        "\n",
        "Frequently the default choice in real-world SVM implementations because of its flexibility and smooth decision boundaries\n",
        "\n",
        "Intuition\n",
        "The RBF kernel implicitly maps data into an infinite-dimensional feature space, capturing very subtle distinctions between points without ever having to compute explicit features\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e40d9f0-842c-4735-8364-a47839ff75b5",
      "metadata": {
        "id": "2e40d9f0-842c-4735-8364-a47839ff75b5"
      },
      "outputs": [],
      "source": [
        "#What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "'''\n",
        "->A Naïve Bayes Classifier is a straightforward yet effective probabilistic classification method grounded in Bayes’ theorem, leveraging the principle of conditional independence among features to make predictions efficiently.\n",
        "\n",
        "Why It’s Called “Naïve”\n",
        "The classifier’s naïvety stems from its strong assumption that features are completely independent of each other, conditional on the class label.\n",
        "“We assume conditional independence because it makes it easier to compute the probabilities. Even though we know it does not exactly reflect the real world. This is why we call it 'naive'.“\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29eed4bb-b7ae-4f1c-8c70-7faf03369808",
      "metadata": {
        "id": "29eed4bb-b7ae-4f1c-8c70-7faf03369808"
      },
      "outputs": [],
      "source": [
        "#Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.When would you use each one?\n",
        "\n",
        "'''\n",
        "->1. Gaussian Naïve Bayes\n",
        "Model assumption: Each feature (predictor) is continuous and follows a Gaussian (normal) distribution within each class. You estimate mean and variance per feature per class.\n",
        "Use case: Ideal for datasets with numeric measurements—e.g. height, weight, temperature, sensor data, or the classic Iris dataset.\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "Model assumption: Features are discrete counts or frequencies, assumed to follow a multinomial distribution. Likelihood is based on counts raised to power of their probabilities.\n",
        "Use case: Very common in text classification (spam detection, sentiment analysis, topic categorization), where features are word counts or term frequencies. It also applies to any other scenario modeled with counts.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "Model assumption: Features are binary (0/1), modeled as independent Bernoulli distributions. It explicitly considers presence versus absence.\n",
        "Use case: Well-suited for binary-valued text features (e.g. word occurs or doesn't in a document), or any scenario with Boolean-type predictors.\n",
        "\n",
        " Choosing the Right Variant\n",
        "If your features are continuous numeric values, go with Gaussian Naïve Bayes.\n",
        "\n",
        "If your features represent counts or frequencies, especially in NLP or document classification, use Multinomial NB.\n",
        "\n",
        "If your features are binary (yes/no, true/false), such as word presence, choose Bernoulli NB.\n",
        "\n",
        "Rare cases where none fits perfectly (e.g., TF‑IDF continuous features), people often still use Multinomial NB or fall back to Gaussian—but careful validation is recommended.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43639080-d521-43f9-b7a9-429cbfd778f2",
      "metadata": {
        "id": "43639080-d521-43f9-b7a9-429cbfd778f2",
        "outputId": "7fdb384b-cc41-42de-8aa2-f023b80d844e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the SVM model: 1.00\n",
            "Support vectors (coordinates):\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "Number of support vectors for each class:\n",
            "[ 3 11 10]\n",
            "Indices of support vectors:\n",
            "[ 16  18  76   7  30  39  44  45  47  58  64  65  90  95   1  15  27  53\n",
            "  66  72  86  97  98 101]\n"
          ]
        }
      ],
      "source": [
        "#Write a Python program to:● Load the Iris dataset● Train an SVM Classifier with a linear kernel● Print the model's accuracy and support vectors\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train SVM with linear kernel\n",
        "svm_model = SVC(kernel='linear', random_state=0)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Evaluate accuracy\n",
        "y_pred = svm_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the SVM model: {accuracy:.2f}\")\n",
        "\n",
        "# 5. Get support vectors\n",
        "print(\"Support vectors (coordinates):\")\n",
        "print(svm_model.support_vectors_)\n",
        "print(\"Number of support vectors for each class:\")\n",
        "print(svm_model.n_support_)\n",
        "print(\"Indices of support vectors:\")\n",
        "print(svm_model.support_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f357b1-b573-4732-b43d-26ce4753e567",
      "metadata": {
        "id": "53f357b1-b573-4732-b43d-26ce4753e567",
        "outputId": "c2416c57-e3cc-4387-c87d-557ebc3bc7a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.92        63\n",
            "           1       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Write a Python program to:● Load the Breast Cancer dataset● Train a Gaussian Naïve Bayes model● Print its classification report including precision, recall, and F1-score\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a15f73-1936-4b9e-9dd6-77a3a2d9e2e2",
      "metadata": {
        "id": "e4a15f73-1936-4b9e-9dd6-77a3a2d9e2e2",
        "outputId": "bfc896fa-6081-4455-b005-0ce505779537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best Hyperparameters:  {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Best Cross-Validation Accuracy: 69.47%\n",
            "Test Accuracy: 77.78%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87        19\n",
            "           1       0.83      0.71      0.77        21\n",
            "           2       0.62      0.71      0.67        14\n",
            "\n",
            "    accuracy                           0.78        54\n",
            "   macro avg       0.77      0.77      0.77        54\n",
            "weighted avg       0.79      0.78      0.78        54\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#: Write a Python program to:● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.● Print the best hyperparameters and accuracy.\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=3)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding accuracy score\n",
        "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "print(\"Test Accuracy: {:.2f}%\".format(best_svm.score(X_test, y_test) * 100))\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb52673b-16fc-41e6-b8d8-5d68f8c86899",
      "metadata": {
        "id": "fb52673b-16fc-41e6-b8d8-5d68f8c86899",
        "outputId": "37bf2e72-2a7b-4bc3-8b0b-3044172b8398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC-AUC Score: 0.9607\n"
          ]
        }
      ],
      "source": [
        "#Write a Python program to:● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. usingsklearn.datasets.fetch_20newsgroups).● Print the model's ROC-AUC score for its predictions\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups = datasets.fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Naïve Bayes classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_prob = nb.predict_proba(X_test)\n",
        "\n",
        "# Compute the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bf1c38-c2f2-4394-86e5-75b4d8e637a3",
      "metadata": {
        "id": "49bf1c38-c2f2-4394-86e5-75b4d8e637a3"
      },
      "outputs": [],
      "source": [
        "# Imagine you’re working as a data scientist for a company that handles email communications.Your task is to automatically classify emails as Spam or Not Spam. The emails maycontain:● Text with diverse vocabulary● Potential class imbalance (far more legitimate emails than spam)● Some incomplete or missing dataExplain the approach you would take to:● Preprocess the data (e.g. text vectorization, handling missing data)● Choose and justify an appropriate model (SVM vs. Naïve Bayes)● Address class imbalance● Evaluate the performance of your solution with suitable metricsAnd explain the business impact of your solution.\n",
        "\n",
        "'''\n",
        "->1. Data Preprocessing\n",
        "Text Vectorization:\n",
        "\n",
        "TF-IDF Vectorizer: Converts text into numerical features by evaluating the importance of words in the documents. It reduces the weight of commonly used words and highlights more informative terms.\n",
        "Handling Missing Data:\n",
        "Imputation: Replace missing email content with empty strings to maintain data consistency and ensure the model can process all entries.\n",
        "Text Cleaning:\n",
        "Lowercasing: Standardizes text by converting all characters to lowercase.\n",
        "Stopword Removal: Eliminates common words (e.g., \"the\", \"is\") that don't contribute to distinguishing spam.\n",
        "2. Model Selection\n",
        "Naïve Bayes Classifier:\n",
        "Multinomial Naïve Bayes is effective for text classification tasks. It assumes feature independence and is computationally efficient.\n",
        "Performance: Achieves high accuracy (~98%) in spam detection tasks.\n",
        "Support Vector Machine (SVM):\n",
        "Linear SVM is suitable for high-dimensional spaces, like text data.\n",
        "3. Addressing Class Imbalance\n",
        "SMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class (spam) to balance the dataset.\n",
        "4. Evaluation Metrics\n",
        "Accuracy: Measures the overall correctness of the model.\n",
        "Precision: Indicates the proportion of true positives among all positive predictions.\n",
        "5. Business Impact\n",
        "Cost Reduction: Automates spam detection, reducing the need for manual intervention.\n",
        "\n",
        "Improved Productivity: Ensures employees focus on legitimate emails, enhancing efficiency.\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b2e021-64cf-4824-b0ad-f17566fc1208",
      "metadata": {
        "id": "64b2e021-64cf-4824-b0ad-f17566fc1208"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e903c531-9bac-440f-b9aa-7cba426788ab",
      "metadata": {
        "id": "e903c531-9bac-440f-b9aa-7cba426788ab"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}